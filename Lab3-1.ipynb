{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Part 1: Classification and Grid Search (33 marks)\n",
    "### Due Date: Monday, March 13 at 12pm\n",
    "\n",
    "Author: *YOUR NAME*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this portion of the assignment is to practice following the grid-search workflow: \n",
    "- Split data into training and test set\n",
    "- Use the training portion to find the best model using grid search and cross-validation\n",
    "- Retrain the best model\n",
    "- Evaluate the retrained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_grid_search_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Function definitions (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "def get_classifier_cv_score(model, X, y, scoring='accuracy', cv=7):\n",
    "    '''Calculate train and validation scores of classifier (model) using cross-validation\n",
    "        \n",
    "        \n",
    "        model (sklearn classifier): Classifier to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        scoring (str): a scoring string accepted by sklearn.metrics.cross_validate()\n",
    "        cv (int): number of cross-validation folds see sklearn.metrics.cross_validate()\n",
    "        \n",
    "        returns: mean training score, mean validation score\n",
    "    \n",
    "    '''\n",
    "    \n",
    "     #TODO: Implement function body\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT NEED TO ADD ANYTHING TO THIS FUNCTION\n",
    "def print_grid_search_result(grid_search):\n",
    "    '''Prints summary of best model from GridSearchCV object.\n",
    "    \n",
    "        For the best model of the grid search, print:\n",
    "        - parameters \n",
    "        - cross-validation training score\n",
    "        - cross-validation validation score \n",
    "        \n",
    "        scores are printed with 3 decimal places\n",
    "    \n",
    "        grid_search (sklearn GridSearchCV): Fitted GridSearchCV object\n",
    "        \n",
    "        returns: None\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    print(grid_search.best_params_)\n",
    "    print(\"training score= {:.3f}; validation score={:.3f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_],\n",
    "                                                                  grid_search.cv_results_['mean_test_score'][grid_search.best_index_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT NEED TO ADD ANYTHING TO THIS FUNCTION\n",
    "import mglearn\n",
    "\n",
    "def plot_grid_search_results(grid_search):\n",
    "    '''For grids with 2 hyperparameters, create a heatmap plot of test scores\n",
    "        \n",
    "        grid_search (sklearn GridSearchCV): Fitted GridSearchCV object\n",
    "        \n",
    "        uses mglearn.tools.heatmap() for plotting\n",
    "    \n",
    "    '''\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    params = sorted(grid_search.param_grid.keys())\n",
    "    \n",
    "    assert len(params) == 2, \"We can only plot two parameters.\"\n",
    "    \n",
    "    # second dimension in reshape are rows, needs to be the fast changing parameter\n",
    "    scores = np.array(results.mean_test_score).reshape(len(grid_search.param_grid[params[0]]),\n",
    "                                                      len(grid_search.param_grid[params[1]]))\n",
    "\n",
    "    # plot the mean cross-validation scores\n",
    "    # x-axis needs to be the fast changing parameter\n",
    "    mglearn.tools.heatmap(scores, \n",
    "                          xlabel=params[1], \n",
    "                          xticklabels=grid_search.param_grid[params[1]], \n",
    "                          ylabel=params[0], \n",
    "                          yticklabels=grid_search.param_grid[params[0]],\n",
    "                          cmap=\"viridis\", fmt=\"%0.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT NEED TO ADD ANYTHING TO THIS FUNCTION\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_actual, y_pred, labels, title):\n",
    "    '''Creates a heatmap plot of the confusion matrix.\n",
    "    \n",
    "        y_actual (pandas.DataSeries or numpy.Array): Ground truth label vector\n",
    "        y_pred (pandas.DataSeries or numpy.Array): Predicted label vector\n",
    "        labels (list(str)): Class names used for plotting (ticklabels)\n",
    "        title (str): Plot title\n",
    "        \n",
    "        uses sklearn.metrics.confusion_matrix\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    mat = confusion_matrix(y_actual, y_pred)\n",
    "\n",
    "    sns.heatmap(mat, \n",
    "                xticklabels=labels,  \n",
    "                yticklabels=labels, \n",
    "                square=True, annot=True, cbar=False, \n",
    "                fmt='d')\n",
    "    plt.xlabel('predicted value')\n",
    "    plt.ylabel('true value')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data (2 marks)\n",
    "yellowbrick mushroom  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/mushroom.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare the feature matrix and target vector\n",
    "\n",
    "Using the yellowbrick `load_mushroom()` function, load the mushroom data set into feature matrix `X` and target vector `y`\n",
    "\n",
    "Print shape and type of `X`, `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add data loading code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing (4 marks)\n",
    "In this dataset, all features are discrete and nominal and need to be encoded. We will use a `OneHotEncoder`\n",
    "\n",
    "The target vector is discrete and nominal as well and contains string labels. While sklearn is OK using string labels in target vectors, you will use a `LabelEncoder` explicitly to convert strings to integers and keep the encoder to translate between the two representations\n",
    "\n",
    "### 2.1 Onehot encoding of features\n",
    "Use `OneHotEncoder` to convert `X` into one-hot-encoded features stored in a variable `X_enc`. Use `sparse=False`. Print shape of `X_enc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add OneHotEncoder here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Label encode target vector \n",
    "Use `LabelEncoder` to encode the target vector `y`, saved in a variable `y_enc`\n",
    "\n",
    "Print the resulting target vector `y_enc` and print the class names available in the `classes_` attribute of the `LabelEncoder` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add LabelEncoder here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create training and test sets (1 mark)\n",
    "Using scikit-learn `train_test_split()` with parameters `random_state=37`, `test_size=0.2`, split `X_enc` and `y_enc` into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add train_test_split() here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare models using cross-validation (4 marks)\n",
    "Create a list containing `LogisticRegression()`, \n",
    "          `SVC()`,\n",
    "          `BernoulliNB()`,\n",
    "        `RandomForestClassifier(random_state=55)`, \n",
    "         `GradientBoostingClassifier(random_state=56)` objects.\n",
    "\n",
    "Iterate this list, then:\n",
    "- Compute the **f1 score** using `get_classifier_cv_score()` with the 7-fold cross-validation defined above with `X_train` and `y_train` as arguments\n",
    "- Print the models' training and validation scores with **3 decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add cross-validation comparison here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter tuning using grid search (8 marks)\n",
    "\n",
    "The inital cross-validation above gives us an idea of how algorithms perfom using their default hyperparameters\n",
    "\n",
    "Next, we will try and improve one model by tuning its hyperparameters using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Grid search for RandomForestClassifier \n",
    "\n",
    "Perform grid search using `GridSearchCV` for the `RandomForestClassifier(n_estimators=300, random_state=55)`:\n",
    "\n",
    "- Set the number of trees `n_estimators=300` for all evaluations\n",
    "- Grid search using 7-fold cross-validation and `f1` as the scoring function \n",
    "- Note that there is a `n_jobs` parameter to run search in parallel. Setting it to the number of CPU cores  or -1 works well\n",
    "\n",
    "Use the following hyperparameters and values:\n",
    "- `'max_depth': [3, 5, 8]`\n",
    "- `'max_features': [0.3, 0.5, 0.7, 0.9]`\n",
    "\n",
    "From sklearn help:\n",
    "> - max_depth: The maximum depth of the tree\n",
    "> - max_features: If float, then max_features is a fraction and max(1, int(max_features * n_features)) features are considered at each split\n",
    "\n",
    "\n",
    "Use the `print_grid_search_result()` and `plot_grid_search_result()` functions defined above to show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup grid search for RandomForestClassifier(random_state=55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform the grid search by calling fit() with X_train and y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Call print_grid_search_result() to print the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 1:* What is the validation score of the best model? Which models in Section 4 does it outperform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADD YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Call plot_grid_search_results() to plot validation metrics for all hyperparameter combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 2:* Where in the hyperparameter ranges does the maximum occur? Would you need to adjust the hyperparameter ranges to find a better model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADD YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Re-train best model (2 marks)\n",
    "Re-train the best `RandomForestClassifier()` from the grid search above on the training dataset `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDOD: Re-train best random forest classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate using test data (4 marks)\n",
    "\n",
    "Using the retrained model and the test dataset `X_test` and `y_test`:\n",
    "- plot the confusion matrix using the `plot_confusion_matrix()` function defined above\n",
    "- print the classification report\n",
    "\n",
    "Make sure the plot has class labels and a title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: call plot_confusion_matrix(), use LabelEncoder classes_ to get labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: print classification report using default threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 3:* What are the precision, recall and accuracy for the positive class on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADD YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 4:* How many false negatives and false positives does the model produce on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADD YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion (4 marks)\n",
    "\n",
    "In the conclusion, comment on the following items and use data to support your findings:\n",
    "1. In the classification of edible vs poisonous mushrooms, what do false positive and false negative predictions imply and what are the consequences of making these mistakes?\n",
    "1. In the classification of edible vs poisonous mushrooms, why is high recall desired?\n",
    "1. Reflect on the usefulness of this model if it would be put into production and propose ways to improve the model and/or application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating while working on this assignment \n",
    "\n",
    "Can include thoughts from Lab 3-2 as well\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
